{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>What's Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole drama is for learning $Q(a_i, S_i)$\n",
    "\n",
    "The simplest form of representing this function is using a table. \n",
    "A table of dimension $(#states, #actions)$. Each cell in the table denotes the value of being in a state $s$ and taking an action $a$ in $s$\n",
    "\n",
    "## Frozen lake env in Gym\n",
    "To establish and learn that kind of Q-table, Frozen lake env in Gym would be best fit.\n",
    "This env is 4x4 grid, each cell can be a stable block, a hole or a goal position. \n",
    "Reward to be in any cell except goal cell is zero. Goal cell carries a reward of one.\n",
    "\n",
    "Now possible states in this env is one of the cells, so, 16.\n",
    "Similarly possible actions (left, right, top, bottom) are 4.\n",
    "Let's dive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-21 01:58:51,712] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#states = 16\n",
      "#actions = 4\n"
     ]
    }
   ],
   "source": [
    "print \"#states =\", env.observation_space.n\n",
    "print \"#actions =\", env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward @ episode 100 = 0.00\n",
      "Reward @ episode 200 = 0.02\n",
      "Reward @ episode 300 = 0.04\n",
      "Reward @ episode 400 = 0.06\n",
      "Reward @ episode 500 = 0.09\n",
      "Reward @ episode 600 = 0.11\n",
      "Reward @ episode 700 = 0.14\n",
      "Reward @ episode 800 = 0.17\n",
      "Reward @ episode 900 = 0.21\n",
      "Reward @ episode 1000 = 0.24\n",
      "Reward @ episode 1100 = 0.28\n",
      "Reward @ episode 1200 = 0.31\n",
      "Reward @ episode 1300 = 0.34\n",
      "Reward @ episode 1400 = 0.37\n",
      "Reward @ episode 1500 = 0.41\n",
      "Reward @ episode 1600 = 0.45\n",
      "Reward @ episode 1700 = 0.49\n",
      "Reward @ episode 1800 = 0.52\n",
      "Reward @ episode 1900 = 0.56\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Init Q table (Value function)\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Now train the env by processing rewards and learn the Q function\n",
    "episodes = 2000\n",
    "learning_rate = 0.80\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes = []\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    \n",
    "    for _ in range(99):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        action = np.argmax(Q[s, :] + (np.random.randn(1, env.action_space.n)*(1./(1 + i))))\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q[s, action] =   Q[s, action] + learning_rate * (reward + (discount_fact * np.max(Q[new_state, :])))\n",
    "                       - learning_rate * Q[s, action]\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes.append(accumulated_reward_for_episode)\n",
    "    if i % 100 == 0:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "    \n",
    "print \"TRAINING ENDED\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.43567931e-01   1.29953830e-02   1.37789988e-02   1.32270488e-02]\n",
      " [  7.30154553e-04   8.34075080e-04   3.19486029e-04   2.26955196e-01]\n",
      " [  2.92785670e-03   9.91106073e-02   4.55518449e-03   4.24503566e-03]\n",
      " [  6.06637322e-04   5.89402814e-04   0.00000000e+00   4.19002118e-02]\n",
      " [  3.60335085e-01   3.60273873e-03   8.06561456e-03   3.68616199e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.28962214e-04   1.14934259e-04   5.71909935e-02   4.39082815e-06]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   5.65256390e-03   4.06607033e-01]\n",
      " [  3.66698844e-03   2.41311287e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  7.74143483e-01   2.15206745e-04   0.00000000e+00   1.26652371e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   7.30694621e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.70226393e-02   9.92924781e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All good, but what if the env is complex?\n",
    "In a way, that it has waay too many states and waaay too many possible actions?\n",
    "\n",
    "Nothing, there will be a bigger table.\n",
    "But, we have a better method to represent that kind of a big table \n",
    "\n",
    "We will train a Neuralnet to learn this Value function by feeding reward converted as target Q values.\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should be the NeuralNet(NN) architecture? \n",
    "\n",
    "In the above code, we were trying to retrieve value benifits of all possible actions given a state $(s)$. \n",
    "So, this NN should accept state as input and output values (probabilities) of taking each of the four actions.\n",
    "That means, NN will have an input layer of $(1x16)$ and output layer of $(1x4)$. \n",
    "\n",
    "Let there just be one layer in between, my hardware is too slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-21 15:52:04,272] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Percentage completion = 17.80\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Let's first define NN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.placeholder(shape=(1, 16), dtype=tf.float32)\n",
    "\n",
    "# Sandwich layer (1, 16)\n",
    "layer_W = tf.Variable(tf.random_uniform((16, 32), 0, 0.1, dtype=tf.float32))\n",
    "layer_b = tf.Variable(tf.zeros((1, 32), dtype=tf.float32))\n",
    "layer = tf.matmul(inputs, layer_W)\n",
    "\n",
    "# Output layer\n",
    "output_W = tf.Variable(tf.random_uniform((16,4), 0, 0.1), dtype=tf.float32)\n",
    "output_b = tf.Variable(tf.zeros((1, 4), dtype=tf.float32))\n",
    "output = tf.matmul(inputs, output_W)\n",
    "Q = output\n",
    "get_action = tf.argmax(Q, 1)\n",
    "\n",
    "# Error function\n",
    "Q_target = tf.placeholder(shape=(1, 4), dtype=tf.float32)\n",
    "error = Q_target - Q\n",
    "reduced_mean_square_error = tf.reduce_sum(tf.square(error))  # loss function\n",
    "\n",
    "# Minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "back_propagate = optimizer.minimize(reduced_mean_square_error)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "sess = tf.Session()\n",
    "init_global_vars = tf.global_variables_initializer()\n",
    "\n",
    "episodes = 2000\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes = []\n",
    "sess.run(init_global_vars)\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    \n",
    "    for _ in range(99):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        # The following lines are that stupidity that kills your PC\n",
    "#        s_one_hot = tf.one_hot(indices=[s], depth=16)\n",
    "#        s_one_hot = sess.run(s_one_hot)\n",
    "        s_one_hot = np.zeros((1, 16))\n",
    "        s_one_hot[0, s] = 1\n",
    "        action, Q_s = sess.run([get_action, Q], {inputs: s_one_hot})\n",
    "        if np.random.rand(1) < 0.15:\n",
    "                action[0] = env.action_space.sample()\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action[0])\n",
    "        # These too! \n",
    "#        env.render()\n",
    "#        new_state_one_hot = tf.one_hot(indices=[new_state], depth=16)\n",
    "#        new_state_one_hot = sess.run(new_state_one_hot)\n",
    "        new_state_one_hot = np.zeros((1, 16))\n",
    "        new_state_one_hot[0, new_state] = 1\n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q_new = sess.run(Q, {inputs: new_state_one_hot})\n",
    "        Q_targeted = Q_s\n",
    "        Q_targeted[0, action[0]] = reward + (discount_fact * np.max(Q_new))\n",
    "        sess.run(back_propagate, {Q_target: Q_targeted, inputs: s_one_hot })\n",
    "#        print sess.run(error, {Q_target: Q_targeted, inputs: s_one_hot })\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes.append(accumulated_reward_for_episode)\n",
    "    if i % 1 == 0 and False:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "print \"Percentage completion = %.2f\" % ((sum(rewards_over_episodes) * 100.0)/ episodes)\n",
    "print \"TRAINING ENDED\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.09951197,  0.12108366,  0.10376973,  0.15691705]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse what this Agent has learned\n",
    "env.render()\n",
    "sess.run(Q, {inputs: [[0, 1, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f988e8db210>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9894a80390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(rewards_over_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Try other architectures and see how runs-to-learn is changing \n",
    "2. Reduce exploration as we learn better \n",
    "3. Modify env.render() to print inline (Easy to visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-Pole env in Gym\n",
    "\n",
    "Again, here we learn the Q-function; But this env demands a little more sophistication. Cart-Pole is the art of balancing a pole which initially stands in upright position; for as long as possible. For every time step the cart is in upright position, we get a reward +1, the episode ends when the cart is >2.4 units from center or pole falls over beyond 15 degrees from vertical.\n",
    "\n",
    "Here observations are of `Box` class.\n",
    "\n",
    "In *Frozen-Lake* problem, we explicitly didn't care about how our actions affected in long run. Let's try that here first! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-25 14:23:58,230] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage completion = 2546.80\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Let's first define NN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Input layer (A Box with 4 entries)\n",
    "inputs = tf.placeholder(shape=(1, 4), dtype=tf.float32)\n",
    "\n",
    "# Sandwich layer (4, 8)\n",
    "layer_W = tf.Variable(tf.random_uniform((4, 8), 0, 0.1, dtype=tf.float32))\n",
    "layer_b = tf.Variable(tf.zeros((1, 32), dtype=tf.float32))\n",
    "layer = tf.matmul(inputs, layer_W)\n",
    "\n",
    "# Output layer (Two possible actions)\n",
    "output_W = tf.Variable(tf.random_uniform((8,2), 0, 0.1), dtype=tf.float32)\n",
    "output_b = tf.Variable(tf.zeros((1, 4), dtype=tf.float32))\n",
    "output = tf.matmul(layer, output_W)\n",
    "Q = output\n",
    "get_action = tf.argmax(Q, 1)\n",
    "\n",
    "# Error function\n",
    "Q_target = tf.placeholder(shape=(1, 2), dtype=tf.float32)\n",
    "error = Q_target - Q\n",
    "reduced_mean_square_error = tf.reduce_sum(tf.square(error))  # loss function\n",
    "\n",
    "# Minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "back_propagate = optimizer.minimize(reduced_mean_square_error)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "sess = tf.Session()\n",
    "init_global_vars = tf.global_variables_initializer()\n",
    "\n",
    "episodes = 2000\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes_1 = []\n",
    "sess.run(init_global_vars)\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    s = s.reshape(1, 4)\n",
    "    \n",
    "    for _ in range(99):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        # The following lines are that stupidity that kills your PC\n",
    "#        s_one_hot = tf.one_hot(indices=[s], depth=16)\n",
    "#        s_one_hot = sess.run(s_one_hot)\n",
    "\n",
    "        action, Q_s = sess.run([get_action, Q], {inputs: s})\n",
    "        if np.random.rand(1) < 0.15:\n",
    "                action[0] = env.action_space.sample()\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action[0])\n",
    "        # env.render()\n",
    "        new_state = new_state.reshape(1, 4)\n",
    "        # These too! \n",
    "        env.render()\n",
    "#        new_state_one_hot = tf.one_hot(indices=[new_state], depth=16)\n",
    "#        new_state_one_hot = sess.run(new_state_one_hot)\n",
    "\n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q_new = sess.run(Q, {inputs: new_state})\n",
    "        Q_targeted = Q_s\n",
    "        Q_targeted[0, action[0]] = reward + (discount_fact * np.max(Q_new))\n",
    "        sess.run(back_propagate, {Q_target: Q_targeted, inputs: s })\n",
    "#        print sess.run(error, {Q_target: Q_targeted, inputs: s_one_hot })\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            time.sleep(0.5)\n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes_1.append(accumulated_reward_for_episode)\n",
    "    if i % 1 == 0 and False:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "print \"Percentage completion = %.2f\" % ((sum(rewards_over_episodes) * 100.0)/ episodes)\n",
    "print \"TRAINING ENDED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f48005751d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niPicgl5ExOcU9CIiPqegFxHxuf8PmsezWQylR1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4800575b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(rewards_over_episodes)\n",
    "plt.plot(rewards_over_episodes_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran two experiments, the blue one with 5000 episodes and the orange with 2000 episodes. \n",
    "\n",
    "It did learn that moving in opposite direction of pole declination will gain a reward. \n",
    "\n",
    "However, it isn't understanding that continuing in the same path will end the run. Can't blame the agent, it doesn't get a negative reward when it ends (Should we include that part? :D )\n",
    "\n",
    "Let's do that below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-25 14:46:41,577] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage completion = 2546.80\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Let's first define NN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Input layer (A Box with 4 entries)\n",
    "inputs = tf.placeholder(shape=(1, 4), dtype=tf.float32)\n",
    "\n",
    "# Sandwich layer (4, 8)\n",
    "layer_W = tf.Variable(tf.random_uniform((4, 8), 0, 0.1, dtype=tf.float32))\n",
    "layer_b = tf.Variable(tf.zeros((1, 32), dtype=tf.float32))\n",
    "layer = tf.matmul(inputs, layer_W)\n",
    "\n",
    "# Output layer (Two possible actions)\n",
    "output_W = tf.Variable(tf.random_uniform((8,2), 0, 0.1), dtype=tf.float32)\n",
    "output_b = tf.Variable(tf.zeros((1, 4), dtype=tf.float32))\n",
    "output = tf.matmul(layer, output_W)\n",
    "Q = output\n",
    "get_action = tf.argmax(Q, 1)\n",
    "\n",
    "# Error function\n",
    "Q_target = tf.placeholder(shape=(1, 2), dtype=tf.float32)\n",
    "error = Q_target - Q\n",
    "reduced_mean_square_error = tf.reduce_sum(tf.square(error))  # loss function\n",
    "\n",
    "# Minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "back_propagate = optimizer.minimize(reduced_mean_square_error)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "sess = tf.Session()\n",
    "init_global_vars = tf.global_variables_initializer()\n",
    "\n",
    "episodes = 2000\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes_2 = []\n",
    "sess.run(init_global_vars)\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    s = s.reshape(1, 4)\n",
    "    \n",
    "    for _ in range(500):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        # The following lines are that stupidity that kills your PC\n",
    "#        s_one_hot = tf.one_hot(indices=[s], depth=16)\n",
    "#        s_one_hot = sess.run(s_one_hot)\n",
    "\n",
    "        action, Q_s = sess.run([get_action, Q], {inputs: s})\n",
    "        if np.random.rand(1) < 0.15:\n",
    "                action[0] = env.action_space.sample()\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action[0])\n",
    "        # env.render()\n",
    "        new_state = new_state.reshape(1, 4)\n",
    "        # These too! \n",
    "#        env.render()\n",
    "#        new_state_one_hot = tf.one_hot(indices=[new_state], depth=16)\n",
    "#        new_state_one_hot = sess.run(new_state_one_hot)\n",
    "\n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q_new = sess.run(Q, {inputs: new_state})\n",
    "        Q_targeted = Q_s\n",
    "        Q_targeted[0, action[0]] = reward + (discount_fact * np.max(Q_new))\n",
    "        sess.run(back_propagate, {Q_target: Q_targeted, inputs: s })\n",
    "#        print sess.run(error, {Q_target: Q_targeted, inputs: s_one_hot })\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            # time.sleep(0.5)\n",
    "            reward = -100\n",
    "            Q_new = sess.run(Q, {inputs: new_state})\n",
    "            Q_targeted[0, action[0]] = reward + (discount_fact * np.max(Q_new))\n",
    "            sess.run(back_propagate, {Q_target: Q_targeted, inputs: s })\n",
    "            # Including a fake negative reward for screwing the goal\n",
    "            \n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes_2.append(accumulated_reward_for_episode)\n",
    "    if i % 1 == 0 and False:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "print \"Percentage completion = %.2f\" % ((sum(rewards_over_episodes) * 100.0)/ episodes)\n",
    "print \"TRAINING ENDED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f47fbefe490>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb4wxBeb/AxUoZWjqM+6uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47fbefe450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(rewards_over_episodes)\n",
    "plt.plot(rewards_over_episodes_2)\n",
    "\n",
    "# Nope, nothing! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider taking an experience into account and see what that does to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 16:37:48,358] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated reward until episode: 0 is 9.00\n",
      "Aggregated reward until episode: 500 is 9.33\n",
      "Aggregated reward until episode: 1000 is 9.35\n",
      "Aggregated reward until episode: 1500 is 9.43\n",
      "Aggregated reward until episode: 2000 is 9.26\n",
      "Aggregated reward until episode: 2500 is 9.35\n",
      "Aggregated reward until episode: 3000 is 9.44\n",
      "Aggregated reward until episode: 3500 is 9.31\n",
      "Aggregated reward until episode: 4000 is 9.39\n",
      "Aggregated reward until episode: 4500 is 9.44\n",
      "Aggregated reward until episode: 5000 is 9.26\n",
      "Aggregated reward until episode: 5500 is 9.35\n",
      "Aggregated reward until episode: 6000 is 9.34\n",
      "Aggregated reward until episode: 6500 is 9.45\n",
      "Aggregated reward until episode: 7000 is 9.39\n",
      "Aggregated reward until episode: 7500 is 9.36\n",
      "Aggregated reward until episode: 8000 is 9.35\n",
      "Aggregated reward until episode: 8500 is 9.25\n",
      "Aggregated reward until episode: 9000 is 9.30\n",
      "Aggregated reward until episode: 9500 is 9.30\n",
      "Aggregated reward until episode: 10000 is 9.28\n",
      "Aggregated reward until episode: 10500 is 9.38\n",
      "Aggregated reward until episode: 11000 is 9.40\n",
      "Aggregated reward until episode: 11500 is 9.30\n",
      "Aggregated reward until episode: 12000 is 9.41\n",
      "Aggregated reward until episode: 12500 is 9.48\n",
      "Aggregated reward until episode: 13000 is 9.38\n",
      "Aggregated reward until episode: 13500 is 9.31\n",
      "Aggregated reward until episode: 14000 is 9.43\n",
      "Aggregated reward until episode: 14500 is 9.23\n",
      "Aggregated reward until episode: 15000 is 9.36\n",
      "Aggregated reward until episode: 15500 is 9.27\n",
      "Aggregated reward until episode: 16000 is 9.37\n",
      "Aggregated reward until episode: 16500 is 9.30\n",
      "Aggregated reward until episode: 17000 is 9.44\n",
      "Aggregated reward until episode: 17500 is 9.40\n",
      "Aggregated reward until episode: 18000 is 9.33\n",
      "Aggregated reward until episode: 18500 is 9.38\n",
      "Aggregated reward until episode: 19000 is 9.31\n",
      "Aggregated reward until episode: 19500 is 9.36\n",
      "Aggregated reward until episode: 20000 is 9.28\n",
      "Aggregated reward until episode: 20500 is 9.44\n",
      "Aggregated reward until episode: 21000 is 9.36\n",
      "Aggregated reward until episode: 21500 is 9.37\n",
      "Aggregated reward until episode: 22000 is 9.39\n",
      "Aggregated reward until episode: 22500 is 9.29\n",
      "Aggregated reward until episode: 23000 is 9.36\n",
      "Aggregated reward until episode: 23500 is 9.31\n",
      "Aggregated reward until episode: 24000 is 9.36\n",
      "Aggregated reward until episode: 24500 is 9.45\n",
      "Aggregated reward until episode: 25000 is 9.34\n",
      "Aggregated reward until episode: 25500 is 9.33\n",
      "Aggregated reward until episode: 26000 is 9.40\n",
      "Aggregated reward until episode: 26500 is 9.41\n",
      "Aggregated reward until episode: 27000 is 9.33\n",
      "Aggregated reward until episode: 27500 is 9.32\n",
      "Aggregated reward until episode: 28000 is 9.44\n",
      "Aggregated reward until episode: 28500 is 9.34\n",
      "Aggregated reward until episode: 29000 is 9.34\n",
      "Aggregated reward until episode: 29500 is 9.29\n",
      "Aggregated reward until episode: 30000 is 9.17\n",
      "Aggregated reward until episode: 30500 is 9.42\n",
      "Aggregated reward until episode: 31000 is 9.25\n",
      "Aggregated reward until episode: 31500 is 9.47\n",
      "Aggregated reward until episode: 32000 is 9.46\n",
      "Aggregated reward until episode: 32500 is 9.42\n",
      "Aggregated reward until episode: 33000 is 9.45\n",
      "Aggregated reward until episode: 33500 is 9.32\n",
      "Aggregated reward until episode: 34000 is 9.35\n",
      "Aggregated reward until episode: 34500 is 9.35\n",
      "Aggregated reward until episode: 35000 is 9.43\n",
      "Aggregated reward until episode: 35500 is 9.46\n",
      "Aggregated reward until episode: 36000 is 9.32\n",
      "Aggregated reward until episode: 36500 is 9.34\n",
      "Aggregated reward until episode: 37000 is 9.35\n",
      "Aggregated reward until episode: 37500 is 9.44\n",
      "Aggregated reward until episode: 38000 is 9.39\n",
      "Aggregated reward until episode: 38500 is 9.45\n",
      "Aggregated reward until episode: 39000 is 9.32\n",
      "Aggregated reward until episode: 39500 is 9.34\n",
      "Aggregated reward until episode: 40000 is 9.33\n",
      "Aggregated reward until episode: 40500 is 9.28\n",
      "Aggregated reward until episode: 41000 is 9.36\n",
      "Aggregated reward until episode: 41500 is 9.45\n",
      "Aggregated reward until episode: 42000 is 9.34\n",
      "Aggregated reward until episode: 42500 is 9.39\n",
      "Aggregated reward until episode: 43000 is 9.29\n",
      "Aggregated reward until episode: 43500 is 9.37\n",
      "Aggregated reward until episode: 44000 is 9.33\n",
      "Aggregated reward until episode: 44500 is 9.34\n",
      "Aggregated reward until episode: 45000 is 9.34\n",
      "Aggregated reward until episode: 45500 is 9.34\n",
      "Aggregated reward until episode: 46000 is 9.28\n",
      "Aggregated reward until episode: 46500 is 9.48\n",
      "Aggregated reward until episode: 47000 is 9.42\n",
      "Aggregated reward until episode: 47500 is 9.23\n",
      "Aggregated reward until episode: 48000 is 9.44\n",
      "Aggregated reward until episode: 48500 is 9.30\n",
      "Aggregated reward until episode: 49000 is 9.39\n",
      "Aggregated reward until episode: 49500 is 9.29\n",
      "Aggregated reward until episode: 50000 is 9.34\n",
      "Aggregated reward until episode: 50500 is 9.31\n",
      "Aggregated reward until episode: 51000 is 9.31\n",
      "Aggregated reward until episode: 51500 is 9.37\n",
      "Aggregated reward until episode: 52000 is 9.26\n",
      "Aggregated reward until episode: 52500 is 9.48\n",
      "Aggregated reward until episode: 53000 is 9.29\n",
      "Aggregated reward until episode: 53500 is 9.41\n",
      "Aggregated reward until episode: 54000 is 9.16\n",
      "Aggregated reward until episode: 54500 is 9.30\n",
      "Aggregated reward until episode: 55000 is 9.34\n",
      "Aggregated reward until episode: 55500 is 9.31\n",
      "Aggregated reward until episode: 56000 is 9.39\n",
      "Aggregated reward until episode: 56500 is 9.23\n",
      "Aggregated reward until episode: 57000 is 9.31\n",
      "Aggregated reward until episode: 57500 is 9.32\n",
      "Aggregated reward until episode: 58000 is 9.37\n",
      "Aggregated reward until episode: 58500 is 9.26\n",
      "Aggregated reward until episode: 59000 is 9.36\n",
      "Aggregated reward until episode: 59500 is 9.32\n",
      "Aggregated reward until episode: 60000 is 9.26\n",
      "Aggregated reward until episode: 60500 is 9.37\n",
      "Aggregated reward until episode: 61000 is 9.32\n",
      "Aggregated reward until episode: 61500 is 9.30\n",
      "Aggregated reward until episode: 62000 is 9.45\n",
      "Aggregated reward until episode: 62500 is 9.26\n",
      "Aggregated reward until episode: 63000 is 9.31\n",
      "Aggregated reward until episode: 63500 is 9.39\n",
      "Aggregated reward until episode: 64000 is 9.42\n",
      "Aggregated reward until episode: 64500 is 9.36\n",
      "Aggregated reward until episode: 65000 is 9.21\n",
      "Aggregated reward until episode: 65500 is 9.35\n",
      "Aggregated reward until episode: 66000 is 9.35\n",
      "Aggregated reward until episode: 66500 is 9.45\n",
      "Aggregated reward until episode: 67000 is 9.46\n",
      "Aggregated reward until episode: 67500 is 9.35\n",
      "Aggregated reward until episode: 68000 is 9.41\n",
      "Aggregated reward until episode: 68500 is 9.33\n",
      "Aggregated reward until episode: 69000 is 9.40\n",
      "Aggregated reward until episode: 69500 is 9.28\n",
      "Aggregated reward until episode: 70000 is 9.20\n",
      "Aggregated reward until episode: 70500 is 9.33\n",
      "Aggregated reward until episode: 71000 is 9.26\n",
      "Aggregated reward until episode: 71500 is 9.42\n",
      "Aggregated reward until episode: 72000 is 9.31\n",
      "Aggregated reward until episode: 72500 is 9.32\n",
      "Aggregated reward until episode: 73000 is 9.42\n",
      "Aggregated reward until episode: 73500 is 9.31\n",
      "Aggregated reward until episode: 74000 is 9.34\n",
      "Aggregated reward until episode: 74500 is 9.36\n",
      "Aggregated reward until episode: 75000 is 9.46\n",
      "Aggregated reward until episode: 75500 is 9.34\n",
      "Aggregated reward until episode: 76000 is 9.34\n",
      "Aggregated reward until episode: 76500 is 9.26\n",
      "Aggregated reward until episode: 77000 is 9.32\n",
      "Aggregated reward until episode: 77500 is 9.55\n",
      "Aggregated reward until episode: 78000 is 9.25\n",
      "Aggregated reward until episode: 78500 is 9.51\n",
      "Aggregated reward until episode: 79000 is 9.17\n",
      "Aggregated reward until episode: 79500 is 9.32\n",
      "Aggregated reward until episode: 80000 is 9.26\n",
      "Aggregated reward until episode: 80500 is 9.33\n",
      "Aggregated reward until episode: 81000 is 9.36\n",
      "Aggregated reward until episode: 81500 is 9.47\n",
      "Aggregated reward until episode: 82000 is 9.37\n",
      "Aggregated reward until episode: 82500 is 9.48\n",
      "Aggregated reward until episode: 83000 is 9.44\n",
      "Aggregated reward until episode: 83500 is 9.43\n",
      "Aggregated reward until episode: 84000 is 9.30\n",
      "Aggregated reward until episode: 84500 is 9.46\n",
      "Aggregated reward until episode: 85000 is 9.43\n",
      "Aggregated reward until episode: 85500 is 9.38\n",
      "Aggregated reward until episode: 86000 is 9.30\n",
      "Aggregated reward until episode: 86500 is 9.39\n",
      "Aggregated reward until episode: 87000 is 9.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated reward until episode: 87500 is 9.41\n",
      "Aggregated reward until episode: 88000 is 9.35\n",
      "Aggregated reward until episode: 88500 is 9.40\n",
      "Aggregated reward until episode: 89000 is 9.28\n",
      "Aggregated reward until episode: 89500 is 9.35\n",
      "Aggregated reward until episode: 90000 is 9.40\n",
      "Aggregated reward until episode: 90500 is 9.41\n",
      "Aggregated reward until episode: 91000 is 9.41\n",
      "Aggregated reward until episode: 91500 is 9.38\n",
      "Aggregated reward until episode: 92000 is 9.33\n",
      "Aggregated reward until episode: 92500 is 9.25\n",
      "Aggregated reward until episode: 93000 is 9.35\n",
      "Aggregated reward until episode: 93500 is 9.35\n",
      "Aggregated reward until episode: 94000 is 9.38\n",
      "Aggregated reward until episode: 94500 is 9.32\n",
      "Aggregated reward until episode: 95000 is 9.34\n",
      "Aggregated reward until episode: 95500 is 9.31\n",
      "Aggregated reward until episode: 96000 is 9.29\n",
      "Aggregated reward until episode: 96500 is 9.22\n",
      "Aggregated reward until episode: 97000 is 9.34\n",
      "Aggregated reward until episode: 97500 is 9.34\n",
      "Aggregated reward until episode: 98000 is 9.40\n",
      "Aggregated reward until episode: 98500 is 9.25\n",
      "Aggregated reward until episode: 99000 is 9.38\n",
      "Aggregated reward until episode: 99500 is 9.42\n",
      "Aggregated reward until episode: 100000 is 9.37\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# MDP layout -> (State, Action, Reward, State_transition_prob, discount_factor)\n",
    "\n",
    "class NeuralAgent(object):\n",
    "    \"\"\" Agent capable of batch training & inference\"\"\"\n",
    "    \n",
    "    def __init__(self, state_space_size, action_space_size, hidden_layer_size, lr):\n",
    "        \n",
    "        # Inference / feedforward\n",
    "        self.input_layer = tf.placeholder(shape=(None, state_space_size), dtype=tf.float32)\n",
    "        hidden_layer = slim.fully_connected(inputs=self.input_layer, num_outputs=hidden_layer_size,\n",
    "                                            activation_fn=tf.nn.relu, biases_initializer=None)\n",
    "        self.output_layer = slim.fully_connected(inputs=hidden_layer, num_outputs=action_space_size, \n",
    "                                           activation_fn=tf.nn.softmax, biases_initializer=None)\n",
    "        \n",
    "        self.chosen_action = tf.argmax(self.output_layer,1)\n",
    "        \n",
    "        \n",
    "        # Loss fn; loss = -reward x log(output_weight)\n",
    "        self.rewards = tf.placeholder(shape=None, dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "        \n",
    "        self.p1 = self.actions\n",
    "        self.p2 = tf.range(0, tf.shape(self.output_layer)[0])\n",
    "        self.p3 = tf.range(0, tf.shape(self.output_layer)[0]) * tf.shape(self.output_layer)[1]\n",
    "        self.p4 = tf.range(0, tf.shape(self.output_layer)[0]) * tf.shape(self.output_layer)[1] + self.actions\n",
    "        \n",
    "        self.indices = tf.range(0, tf.shape(self.output_layer)[0]) * tf.shape(self.output_layer)[1] + self.actions\n",
    "        self.related_weights = tf.gather(tf.reshape(self.output_layer, [-1]), indices=self.indices)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.related_weights) * self.rewards)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.train = optimizer.minimize(self.loss)\n",
    "        \n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
    "    \"\"\"\n",
    "    return np.array([val * (gamma ** i) for i, val in enumerate(r)])\n",
    "        \n",
    "        \n",
    "        \n",
    "episodes = 100002\n",
    "update_rate = 5\n",
    "max_episode_length = 1000 / update_rate\n",
    "reward_list = []\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = NeuralAgent(state_space_size=4, action_space_size=2, hidden_layer_size=8, lr=0.01)\n",
    "accumulated_reward = []\n",
    "\n",
    "# Gotcha \"Initialize variables after forming the AdamOpt graph \"\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run(tf.local_variables_initializer())\n",
    "for i in range(episodes):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    s = env.reset() \n",
    "    s = s.reshape(1, 4)\n",
    "    \n",
    "    for _ in range(max_episode_length):\n",
    "        \n",
    "        # Accumulating experience\n",
    "        episode_history = []\n",
    "        for _ in range(update_rate):\n",
    "    \n",
    "            a_dist = sess.run(agent.output_layer, feed_dict={agent.input_layer: s})\n",
    "            a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "            # todo; implement greedy selection\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action=action)\n",
    "            new_state = new_state.reshape(1, 4)\n",
    "            episode_history.append([s[0].tolist(), action, reward])\n",
    "            \n",
    "            episode_reward += reward\n",
    "            s = new_state\n",
    "            if done is True:\n",
    "                break\n",
    "        \n",
    "        # Updating weights\n",
    "        episode_history = zip(*episode_history)\n",
    "        rewards_ = discount_rewards(np.array(episode_history[2])).reshape(-1, 1)\n",
    "        actions_ = np.array(episode_history[1]).reshape(-1, 1)\n",
    "        states_ = np.array(episode_history[0]).reshape(-1, 4)\n",
    "        \n",
    "        feed_dict = {agent.input_layer: states_, agent.rewards: rewards_, agent.actions: actions_}\n",
    "        \n",
    "#        for state_, reward_, action_ in zip(states_, rewards_, actions_):\n",
    "#            feed_dict = {agent.input_layer: state_, agent.rewards: reward_, agent.actions: action_}\n",
    "#            sess.run(agent.train, feed_dict=feed_dict)\n",
    "        \n",
    "        sess.run(agent.train, feed_dict=feed_dict)\n",
    "#        sess.run(agent.loss, feed_dict)\n",
    "        \n",
    "        if done is True:\n",
    "            accumulated_reward.append(episode_reward)\n",
    "            break\n",
    "            \n",
    "    if i % 500 == 0:\n",
    "        try:\n",
    "            print \"Aggregated reward until episode: %d is %.2f\" % (i, np.mean(accumulated_reward[-100:]))\n",
    "        \n",
    "        except:\n",
    "            print \"Aggregated reward until episode: %d is %.2f\" % (i, sum(accumulated_reward) / (i + 1.0))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what happened there... \n",
    "It had been a few months since I was on this cart-pole.\n",
    "\n",
    "And I've learnt Keras. \n",
    "So, I will be using Keras API with TF/ Theano (R.I.P) as a backend.\n",
    "\n",
    "Alright, target is to record buffer of experiences and learn on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-22 16:36:48,097] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accumulated reward 13.0\n",
      "Average accumulated reward 9.39603960396\n",
      "Average accumulated reward 9.32835820896\n",
      "Average accumulated reward 9.28571428571\n",
      "Average accumulated reward 9.29177057357\n",
      "Average accumulated reward 9.28343313373\n",
      "Average accumulated reward 9.30116472546\n",
      "Average accumulated reward 9.31954350927\n",
      "Average accumulated reward 9.31835205993\n",
      "Average accumulated reward 9.33296337403\n",
      "Average accumulated reward 9.34365634366\n",
      "Average accumulated reward 9.35876475931\n",
      "Average accumulated reward 9.36303080766\n",
      "Average accumulated reward 9.36587240584\n",
      "Average accumulated reward 9.35760171306\n",
      "Average accumulated reward 9.35776149234\n",
      "Average accumulated reward 9.35665209244\n",
      "Average accumulated reward 9.35508524397\n",
      "Average accumulated reward 9.35702387562\n",
      "Average accumulated reward 9.35718043135\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def build_neural_agent(input_dim):\n",
    "\n",
    "    input_ = Input(shape=(4, 1))\n",
    "    # reward = Input(shape=(1,))\n",
    "    x = Dense(4, activation=\"relu\")(input_)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(8, activation=\"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    output = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    agent = Model(inputs=input_, outputs=output)\n",
    "    agent.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def build_agent():\n",
    "\n",
    "    agent = build_neural_agent((1, 4, 1))\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def get_action_proba(agent, state_, reward=0):\n",
    "\n",
    "    action_proba = agent.predict(state_)\n",
    "\n",
    "    return action_proba\n",
    "\n",
    "\n",
    "def get_next_action(agent, state_, reward, greedy=False):\n",
    "\n",
    "    action_proba = get_action_proba(reward=reward,\n",
    "                                    state_=state_.reshape(1, 4, 1),\n",
    "                                    agent=agent)\n",
    "    # TODO implement greedy action picking\n",
    "\n",
    "    return int(np.argmax(action_proba))\n",
    "\n",
    "\n",
    "def sync_reward(reward, action):\n",
    "\n",
    "    if reward > 0:\n",
    "        y_true = np.array([0.0, 0.0])\n",
    "        y_true[action] = 1.0\n",
    "    else:\n",
    "        y_true = np.array([1.0, 1.0])\n",
    "        y_true[action] = 0.0\n",
    "\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def simple_exploration():\n",
    "\n",
    "    episodes = 2000\n",
    "    max_episode_len = 500\n",
    "    buffer_len = 50\n",
    "\n",
    "    # Init env\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    agent = build_agent()\n",
    "    reward = 0\n",
    "\n",
    "    avg_reward_over_episodes = []\n",
    "    for episode_num in range(episodes):\n",
    "\n",
    "        current_state = env.reset()\n",
    "        total_reward = 0\n",
    "        buffer_a = []\n",
    "        buffer_r = []\n",
    "        for _ in range(max_episode_len):\n",
    "            action = get_next_action(state_=current_state,\n",
    "                                     agent=agent,\n",
    "                                     reward=reward)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            total_reward += reward\n",
    "\n",
    "            # How do we communicate reward with agent? TODO\n",
    "            y_true = sync_reward(reward=reward, action=action)\n",
    "\n",
    "            buffer_a.append(current_state)\n",
    "            buffer_r.append(y_true.reshape(-1, 2))\n",
    "            if len(buffer_a) >= buffer_len or done is True:\n",
    "                buffer_a, buffer_r = np.array(buffer_a), np.array(buffer_r)\n",
    "                agent.fit(x=buffer_a.reshape(-1, 4, 1),\n",
    "                          y=buffer_r.reshape(-1, 2),\n",
    "                          verbose=False)\n",
    "\n",
    "                buffer_a, buffer_r = [], []\n",
    "\n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        avg_reward_over_episodes.append(total_reward)\n",
    "\n",
    "        if episode_num % 100 == 0:\n",
    "            print(\"Average accumulated reward {}\".format(\n",
    "                np.array(avg_reward_over_episodes).mean())\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simple_exploration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
